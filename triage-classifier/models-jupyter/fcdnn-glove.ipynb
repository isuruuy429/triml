{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "!pip install sequential\n",
    "!pip install tensorflow keras"
   ],
   "id": "c93180c24dc885c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import pickle\n",
    "import gensim.downloader as api\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.layers import Input\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ],
   "id": "7e9a5842bef992be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Path to the file\n",
    "file_path = '../data/ED-triage-obs-final.xlsx'  # Update with your local file path\n",
    "df = pd.read_excel(file_path)"
   ],
   "id": "c2c7d851fa3d639d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Drop duplicate rows\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Initialize NLTK resources\n",
    "stop_words = set(stopwords.words('english')) - {\"no\", \"not\", \"wasn't\", \"was not\", \"isn't\", \"is not\"}\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = text.lower()  # Lowercasing\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
    "    words = word_tokenize(text)  # Tokenization\n",
    "    words = [word for word in words if word not in stop_words]  # Remove stopwords\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]  # Lemmatization\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df = df.drop(columns=[\"Blood Glucose, Capillary\", \"Departed\", \"Arrived\", \"Diastolic Blood Pressure\", \"Departure Status\", \"Respiratory Rate\", \"Temperature Tympanic\"])\n",
    "\n",
    "# Drop rows with missing 'Triage', 'Chief Complaint', 'Visit Reason', and any of the vital signs\n",
    "df.dropna(subset=['Triage', 'Chief Complaint', 'Visit Reason', 'SpO2', 'Peripheral Pulse Rate', 'Systolic Blood Pressure'], inplace=True)\n"
   ],
   "id": "8aef6fe64ab6f55b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define valid triage levels\n",
    "valid_triage_levels = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Drop rows that do not contain valid triage levels\n",
    "df = df[df['Triage'].isin(valid_triage_levels)]\n",
    "\n",
    "# Preprocess Visit Reason and Chief Complaint\n",
    "df['Visit Reason'] = df['Visit Reason'].apply(preprocess_text)\n",
    "df['Chief Complaint'] = df['Chief Complaint'].apply(preprocess_text)\n",
    "\n",
    "# Combine Visit Reason and Chief Complaint\n",
    "df['combined_text'] = df['Visit Reason'] + ' ' + df['Chief Complaint']"
   ],
   "id": "b78d711191923534",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Step 1: Load Pre-trained GloVe Embeddings\n",
    "glove_model = api.load(\"glove-wiki-gigaword-200\")"
   ],
   "id": "189e6b20c14035ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Step 2: Convert combined_text to a vector using pre-trained GloVe embeddings\n",
    "def get_sentence_embedding_glove(sentence, glove_model):\n",
    "    words = sentence.split()\n",
    "    word_vecs = [glove_model[word] for word in words if word in glove_model]\n",
    "    if len(word_vecs) == 0:\n",
    "        return np.zeros(200)  # GloVe is 200-dimensional\n",
    "    return np.mean(word_vecs, axis=0)\n",
    "\n",
    "df['text_embedding'] = df['combined_text'].apply(lambda x: get_sentence_embedding_glove(x, glove_model))\n",
    "\n",
    "# Extract the vital signs\n",
    "vital_signs = df[['SpO2', 'Peripheral Pulse Rate', 'Systolic Blood Pressure']].values"
   ],
   "id": "d70b189667530ecd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "try:\n",
    "    # Step 3: Scale the text embeddings and vital signs together\n",
    "    text_embeddings = np.vstack(df['text_embedding'].values)\n",
    "    X_combined = np.hstack((text_embeddings, vital_signs))\n",
    "\n",
    "    # Standardize the combined features\n",
    "    scaler = StandardScaler()\n",
    "    X_combined_scaled = scaler.fit_transform(X_combined)\n",
    "\n",
    "    # Save the scaler\n",
    "    scaler_path = \"models3/fcdnn/glove/scaler.pkl\"\n",
    "    with open(scaler_path, \"wb\") as f:\n",
    "        pickle.dump(scaler, f)\n",
    "\n",
    "    # Split the data into features and labels\n",
    "    X = X_combined_scaled\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)"
   ],
   "id": "4d5f60ba129f9093",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "y = df['Triage']",
   "id": "860e28b1d0f9e94f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Visualize data before applying SMOTE \n",
    "df['Triage'] = df['Triage'].astype('category')\n",
    "\n",
    "# Plot the distribution of Triage levels\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='Triage', data=df, hue='Triage', palette='Blues', legend=False)\n",
    "plt.title('Triage Level Distribution before applying SMOTE')\n",
    "plt.xlabel('Triage Level')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ],
   "id": "3bc9518d084d9301",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Apply only SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Plot the distribution after SMOTE\n",
    "resampled_df = pd.DataFrame({'Triage': y_resampled})\n",
    "resampled_df['Triage'] = resampled_df['Triage'].astype('category')\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='Triage', data=resampled_df, hue='Triage', palette='Blues', legend=False)\n",
    "plt.title('Triage Level Distribution After SMOTE')\n",
    "plt.xlabel('Triage Level')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ],
   "id": "50e21ae0172b10a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)"
   ],
   "id": "11f60600918e1032",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Build the FCDNN for classification\n",
    "model = Sequential()\n",
    "\n",
    "# Input layer\n",
    "model.add(Input(shape=(X_train.shape[1],)))\n",
    "\n",
    "# First hidden layer\n",
    "model.add(Dense(128))\n",
    "model.add(LeakyReLU(negative_slope=0.1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Second hidden layer\n",
    "model.add(Dense(64))\n",
    "model.add(LeakyReLU(negative_slope=0.1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Third hidden layer\n",
    "model.add(Dense(32, activation='relu'))\n",
    "\n",
    "# Output layer for multiclass classification (5 classes)\n",
    "model.add(Dense(5, activation='softmax'))  # 5 classes for triage levels"
   ],
   "id": "c1b4d9ca139b7df2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Shift labels to range [0, 4] instead of [1, 5]\n",
    "y_train -= 1\n",
    "y_test -= 1\n",
    "\n",
    "# Compile the model with sparse categorical crossentropy for classification\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Set up callbacks for early stopping and learning rate reduction\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=64,\n",
    "                    validation_split=0.2, verbose=1,\n",
    "                    callbacks=[early_stopping, lr_scheduler])\n",
    "\n",
    "# Save the trained model in .h5 format\n",
    "model_path = \"../models/fcdnn/glove/fcdnn_model.keras\"\n",
    "model.save(model_path)\n",
    "print(f\"Neural Network model saved at {model_path}\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)"
   ],
   "id": "bd89a1af7d87145f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Convert predictions to class labels (shift back to 1-5)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1) + 1\n",
    "\n",
    "# Shift the true labels back to 1-5 as well\n",
    "y_test_shifted = y_test + 1\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test_shifted, y_pred_classes)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_test_shifted, y_pred_classes))"
   ],
   "id": "3cb04f78ad56a16b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Shift the true labels back to the range of 1-5\n",
    "y_test_shifted = y_test + 1\n",
    "\n",
    "# Now, generate the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_test_shifted, y_pred_classes)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', xticklabels=['1', '2', '3', '4', '5'], yticklabels=['1', '2', '3', '4', '5'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n"
   ],
   "id": "9747217ac922c8c3",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
