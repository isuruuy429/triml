{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ],
   "id": "e8d01064ac44312e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load the dataset\n",
    "file_path = '../data/ED-triage-obs-final.xlsx'  # Update with your local file path\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "df.head()"
   ],
   "id": "db8f25f9921e3f00",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Map triage levels to ranges\n",
    "def map_to_range(level):\n",
    "    if level in [1,2]:\n",
    "        return \"Range 1\"\n",
    "    elif level == 3:\n",
    "        return \"Range 2\"\n",
    "    elif level in [4,5]:\n",
    "        return \"Range 3\"\n",
    "\n",
    "# Apply mapping to the triage levels\n",
    "df['Triage-Range'] = df['Triage'].apply(map_to_range)\n",
    "\n",
    "df.head()"
   ],
   "id": "f5f609cf24bdfbed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.shape",
   "id": "8a69731c7ccaa08d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Drop duplicate rows\n",
    "df = df.drop_duplicates()"
   ],
   "id": "81f2ccefae55b494",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Drop unnecessary columns\n",
    "df = df.drop(columns=[\"Blood Glucose, Capillary\", \"Departed\", \"Arrived\", \"Departure Status\", 'Diastolic Blood Pressure', 'Temperature Tympanic', 'Respiratory Rate'])"
   ],
   "id": "a7dd9b354d38e20f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Drop rows with missing 'Triage', 'Chief Complaint', and 'Visit Reason'.\n",
    "df.dropna(subset=['Triage', 'Chief Complaint', 'Visit Reason', 'Systolic Blood Pressure', \"SpO2\", \"Peripheral Pulse Rate\"],inplace=True)"
   ],
   "id": "ded77703933921e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.shape",
   "id": "f378b509582e7297",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialize NLTK resources\n",
    "stop_words = set(stopwords.words('english')) - {\"no\", \"not\", \"wasn't\", \"was not\", \"isn't\", \"is not\"}\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = text.lower()  # Lowercasing\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
    "    words = word_tokenize(text)  # Tokenization\n",
    "    words = [word for word in words if word not in stop_words]  # Remove stopwords\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]  # Lemmatization\n",
    "    return ' '.join(words)"
   ],
   "id": "a86b585a6bb5989c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define valid triage levels\n",
    "valid_triage_levels = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Drop rows that do not contain valid triage levels\n",
    "df = df[df['Triage'].isin(valid_triage_levels)]"
   ],
   "id": "da51525fb28688a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Preprocess Visit Reason and Chief Complaint\n",
    "df['Visit Reason'] = df['Visit Reason'].apply(preprocess_text)\n",
    "df['Chief Complaint'] = df['Chief Complaint'].apply(preprocess_text)\n",
    "\n",
    "# Combine Visit Reason and Chief Complaint\n",
    "df['combined_text'] = df['Visit Reason'] + ' ' + df['Chief Complaint']\n",
    "\n",
    "# Preprocess the combined_text column\n",
    "sentences = df['combined_text'].apply(lambda x: x.split())"
   ],
   "id": "ca648fae8dea997d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Step 2: Train Word2Vec model on the combined_text column\n",
    "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4, seed=42)\n",
    "\n",
    "word2vec_model_path = \"models/word2vec.model\"\n",
    "word2vec_model.save(word2vec_model_path)\n",
    "print(f\"Word2Vec model saved at {word2vec_model_path}\")"
   ],
   "id": "2391387aa3e81348",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Convert combined_text to a vector by averaging word embeddings\n",
    "def get_sentence_embedding(sentence, model):\n",
    "    words = sentence.split()\n",
    "    word_vecs = [model.wv[word] for word in words if word in model.wv]\n",
    "    if len(word_vecs) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean(word_vecs, axis=0)\n",
    "\n",
    "df['text_embedding'] = df['combined_text'].apply(lambda x: get_sentence_embedding(x, word2vec_model))"
   ],
   "id": "de84fdd11974f1a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Extract the vital signs\n",
    "vital_signs = df[['SpO2', 'Peripheral Pulse Rate', 'Systolic Blood Pressure']].values"
   ],
   "id": "dfc3743734694db1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Step 4: Scale the text embeddings and vital signs separately\n",
    "scaler_embeddings = StandardScaler()\n",
    "text_embeddings_scaled = scaler_embeddings.fit_transform(np.vstack(df['text_embedding']))\n",
    "\n",
    "scaler_vitals = StandardScaler()\n",
    "vital_signs_scaled = scaler_vitals.fit_transform(vital_signs)\n",
    "\n",
    "# Save the scalers\n",
    "scaler_embeddings_path = \"models/scaler-embeddings.pkl\"\n",
    "with open(scaler_embeddings_path, \"wb\") as f:\n",
    "    pickle.dump(scaler_embeddings, f)\n",
    "print(f\"Scaler for embeddings saved at {scaler_embeddings_path}\")\n",
    "\n",
    "scaler_vitals_path = \"models/scaler-vitals.pkl\"\n",
    "with open(scaler_vitals_path, \"wb\") as f:\n",
    "    pickle.dump(scaler_vitals, f)\n",
    "print(f\"Scaler for vitals saved at {scaler_vitals_path}\")\n",
    "\n",
    "# Combine the scaled text embeddings and scaled vital signs\n",
    "X = np.hstack((text_embeddings_scaled, vital_signs_scaled))"
   ],
   "id": "eb5dac0dacbfb2eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "y = df['Triage']",
   "id": "ff31ebe1f7bb7edd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Visualize data before applying SMOTE \n",
    "df['Triage'] = df['Triage'].astype('category')\n",
    "\n",
    "# Plot the distribution of Triage levels\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='Triage', data=df, hue='Triage', palette='Blues', legend=False)\n",
    "plt.title('Triage level Distribution before applying SMOTE')\n",
    "plt.xlabel('Triage Level')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ],
   "id": "e64c4c8b1bbb6d8c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Apply SMOTE \n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Plot the distribution after SMOTE\n",
    "resampled_df = pd.DataFrame({'Triage': y_resampled})\n",
    "resampled_df['Triage'] = resampled_df['Triage'].astype('category')\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='Triage', data=resampled_df, hue='Triage', palette='Blues', legend=False)\n",
    "plt.title('Triage Level Distribution After SMOTE')\n",
    "plt.xlabel('Triage Level')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ],
   "id": "b4c74742cba8a95c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Map resampled Triage levels to Triage-Range\n",
    "resampled_df['Triage-Range'] = resampled_df['Triage'].apply(map_to_range)\n",
    "\n",
    "# Update the target variable to the resampled Triage-Range\n",
    "y_resampled_range = resampled_df['Triage-Range']"
   ],
   "id": "fb51353826d67843",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Split data for Triage Range classification\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled_range, test_size=0.2, random_state=42)"
   ],
   "id": "6e91d3be5ada671d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Train the Random Forest Classifier for Triage Range\n",
    "range_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "range_classifier.fit(X_train, y_train)"
   ],
   "id": "6c0cbdc3e0495c36",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Make predictions\n",
    "y_pred = range_classifier.predict(X_test)"
   ],
   "id": "aba3e0bb3230635c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Evaluate the classifier\n",
    "print(\"Classification Report for Triage Range:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Confusion Matrix for Triage Range:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Calculate and display accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Save the range classifier\n",
    "range_model_path = \"models/classifier3.pkl\"\n",
    "with open(range_model_path, \"wb\") as f:\n",
    "    pickle.dump(range_classifier, f)\n",
    "print(f\"Triage Range Random Forest model saved at {range_model_path}\")\n"
   ],
   "id": "5c07469a4b3fef4c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Generate classification report as a dictionary\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "# Convert the classification report to a DataFrame\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "# Filter the report DataFrame to include only the relevant classes\n",
    "filtered_report_df = report_df.loc[['Range 1', 'Range 2', 'Range 3']]\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(filtered_report_df, annot=True, fmt=\".2f\", cmap=\"coolwarm\", cbar=True)\n",
    "plt.title(\"Classification Report Heatmap\")\n",
    "plt.xlabel(\"Metrics\")\n",
    "plt.ylabel(\"Triage Ranges\")\n",
    "plt.show()"
   ],
   "id": "df9d3daa479cb0ca",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
