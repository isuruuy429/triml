{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import pickle\n",
    "import gensim.downloader as api\n",
    "from tensorflow.keras.layers import Input\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.utils import resample"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ],
   "id": "5a19605ed049427a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Path to the file\n",
    "file_path = '../data/ED-triage-obs-final.xlsx'  # Update with your local file path\n",
    "df = pd.read_excel(file_path)\n",
    "df = df[df['Triage'].isin([1, 2])].drop_duplicates()\n",
    "df.shape"
   ],
   "id": "e6301e1f28878b8f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialize NLTK resources\n",
    "stop_words = set(stopwords.words('english')) - {\"no\", \"not\", \"wasn't\", \"was not\", \"isn't\", \"is not\"}\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = text.lower()  # Lowercasing\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
    "    words = word_tokenize(text)  # Tokenization\n",
    "    words = [word for word in words if word not in stop_words]  # Remove stopwords\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]  # Lemmatization\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df = df.drop(columns=[\"Blood Glucose, Capillary\", \"Departed\", \"Arrived\", \"Diastolic Blood Pressure\", \"Departure Status\", \"Respiratory Rate\", \"Temperature Tympanic\"])\n",
    "\n",
    "# Drop rows with missing 'Triage', 'Chief Complaint', 'Visit Reason', and any of the vital signs\n",
    "df.dropna(subset=['Triage', 'Chief Complaint', 'Visit Reason', 'SpO2', 'Peripheral Pulse Rate', 'Systolic Blood Pressure'], inplace=True)"
   ],
   "id": "77cfbd220c92246d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Preprocess Visit Reason and Chief Complaint\n",
    "df['Visit Reason'] = df['Visit Reason'].apply(preprocess_text)\n",
    "df['Chief Complaint'] = df['Chief Complaint'].apply(preprocess_text)\n",
    "\n",
    "# Combine Visit Reason and Chief Complaint\n",
    "df['combined_text'] = df['Visit Reason'] + ' ' + df['Chief Complaint']\n",
    "\n",
    "# Preprocess the combined_text column\n",
    "sentences = df['combined_text'].apply(lambda x: x.split())"
   ],
   "id": "d4311db62d0817c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Step 1: Load Pre-trained GloVe Embeddings\n",
    "glove_model = api.load(\"glove-wiki-gigaword-200\")"
   ],
   "id": "3fa4d39b2b202c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Step 2: Convert combined_text to a vector using pre-trained GloVe embeddings\n",
    "def get_sentence_embedding_glove(sentence, glove_model):\n",
    "    words = sentence.split()\n",
    "    word_vecs = [glove_model[word] for word in words if word in glove_model]\n",
    "    if len(word_vecs) == 0:\n",
    "        return np.zeros(200)  # GloVe is 200-dimensional\n",
    "    return np.mean(word_vecs, axis=0)\n",
    "\n",
    "df['text_embedding'] = df['combined_text'].apply(lambda x: get_sentence_embedding_glove(x, glove_model))\n",
    "\n",
    "# Extract the vital signs\n",
    "vital_signs = df[['SpO2', 'Peripheral Pulse Rate', 'Systolic Blood Pressure']].values"
   ],
   "id": "4af26408d2edb305",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "try:\n",
    "    # Step 3: Scale the text embeddings and vital signs together\n",
    "    text_embeddings = np.vstack(df['text_embedding'].values)\n",
    "    X_combined = np.hstack((text_embeddings, vital_signs))\n",
    "\n",
    "    # Standardize the combined features\n",
    "    scaler = StandardScaler()\n",
    "    X_combined_scaled = scaler.fit_transform(X_combined)\n",
    "\n",
    "    # Save the scaler\n",
    "    scaler_path = \"../models/fcdnn/spe2/scaler.pkl\"\n",
    "    with open(scaler_path, \"wb\") as f:\n",
    "        pickle.dump(scaler, f)\n",
    "\n",
    "    # Split the data into features and labels\n",
    "    X = X_combined_scaled\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)"
   ],
   "id": "1d97f78b11a5ecb9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "y = df['Triage'] - 1",
   "id": "fbf417f809e447a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot class distribution using Seaborn\n",
    "sns.countplot(x='Triage', data=df, palette='pastel')\n",
    "plt.title('Class Distribution Before Embedding')\n",
    "plt.xlabel('Triage Level')\n",
    "plt.ylabel('Number of Instances')\n",
    "plt.show()"
   ],
   "id": "91c0423f46df7d40",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "id": "a6a5b1344bc9eb4d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Combine X_train and y_train into a DataFrame for resampling\n",
    "train_df = pd.DataFrame(X_train)\n",
    "train_df['Triage'] = y_train.values\n",
    "\n",
    "# Separate classes\n",
    "majority = train_df[train_df['Triage'] == 1]\n",
    "minority = train_df[train_df['Triage'] == 0]\n",
    "\n",
    "# Oversample the minority class by duplication\n",
    "minority_oversampled = resample(minority, \n",
    "                                replace=True, \n",
    "                                n_samples=len(majority), \n",
    "                                random_state=42)\n",
    "\n",
    "# Combine oversampled minority with the majority class\n",
    "balanced_train_df = pd.concat([majority, minority_oversampled])\n",
    "\n",
    "X_train_resampled = balanced_train_df.drop(columns=['Triage']).values\n",
    "y_train_resampled = balanced_train_df['Triage'].values\n",
    "\n",
    "sns.countplot(x=y_train_resampled, palette='pastel')\n",
    "plt.title('Class Distribution After Resampling')\n",
    "plt.xlabel('Triage Level')\n",
    "plt.ylabel('Number of Instances')\n",
    "plt.show()"
   ],
   "id": "5e732fd714518595",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Build the FCDNN for classification\n",
    "model = Sequential()\n",
    "\n",
    "# Input layer\n",
    "model.add(Input(shape=(X_train.shape[1],)))\n",
    "\n",
    "# First hidden layer\n",
    "model.add(Dense(128))\n",
    "model.add(LeakyReLU(negative_slope=0.1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Second hidden layer\n",
    "model.add(Dense(64))\n",
    "model.add(LeakyReLU(negative_slope=0.1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Third hidden layer\n",
    "model.add(Dense(32, activation='relu'))\n",
    "\n",
    "# Output layer for multiclass classification\n",
    "model.add(Dense(2, activation='softmax'))  "
   ],
   "id": "2706badf8b1aab0d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Compile the model with sparse categorical crossentropy for classification\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Set up callbacks for early stopping and learning rate reduction\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=64,\n",
    "                    validation_split=0.2, verbose=1,\n",
    "                    callbacks=[early_stopping, lr_scheduler])\n",
    "\n",
    "# Save the trained model in .h5 format\n",
    "model_path = \"../models/fcdnn/spe2/fcdnn_model.keras\"\n",
    "model.save(model_path)\n",
    "print(f\"Neural Network model saved at {model_path}\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)"
   ],
   "id": "d07058b10a395dca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Convert predictions to class labels (shift back to 1-5)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_test, y_pred_classes))\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_classes)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', xticklabels=['Triage 1', 'Triage 2'], yticklabels=['Triage 1', 'Triage 2'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n"
   ],
   "id": "4d2cf3eabd69dbd1",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
